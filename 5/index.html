<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Fun with Diffusion Models</title>
  <link rel="stylesheet" href="https://eecs189.org/fa25/assets/css/style.css" />
  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      line-height: 1.6;
      color: #222;
      max-width: 900px;
      margin: 40px auto;
      padding: 0 20px;
      background: #fff;
    }

    h1 {
      border-bottom: 2px solid #0066cc;
      padding-bottom: 0.3em;
      color: #004080;
    }

    h2 {
      margin-top: 2.5em;
      font-weight: bold;
      color: #003366;
    }

    p {
      margin-top: 0.5em;
    }

    img {
      max-width: 100%;
      height: auto;
      margin-top: 1em;
      display: block;
    }

    .result-image {
      display: inline-block;
      margin: 15px;
      text-align: center;
    }

    .result-image img {
      width: 250px;
      height: auto;
      border: 1px solid #ccc;
    }

    .image-grid {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
    }

    .image-caption {
      margin-top: 0.5em;
      font-size: 0.9em;
    }
  </style>
</head>
<body>

  <header>
    <h1>Fun with Diffusion Models</h1>
  </header>

  <main>

    
    <section>
      <h2>Part A.0: Setup & Text Prompts</h2>
      <p>In part A of the project, we experiment with diffusion models, sampling loops, and use them for tasks such as inpainting and creating optical illusions.</p>
      
      <p>First, we obtain access to the DeepFloyd model and generate our own text prompts which we use to create images. I ran the model using a random seed value of 180 and tried out several different combinations for num_inference_steps using 20 and 200. At num_inference_steps = 20, the outputs were decent but not impressive. It was somewhat obvious that the outputs were AI-generated as they didn't look very realistic (especially due to the colors) and didn't have much texture / detail. Meanwhile, at a value of 200, the outputs had much more detail but the colors still looked somewhat exagerrated, especially for the pasta prompt. For the airport prompt, I think it could have done a better job making it clear that it was an airport in Japan specifically, and for the "man in love" prompt, I think it wasn't super obvious that the person was in love (except for maybe the first output with num_inference_steps = 20) -- it seemed like a normal glance and the expressions didn't look soft. I also tried a num_inference_step value of 400 and interestingly, there were visible small circles in the outputs.</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/part0-20.jpg" style="width:100%; max-width:1600px; height:auto;" alt="part0-20">
          <div class="image-caption">Results for num_inference_steps = 20</div>
        </div>
        
        <div class="result-image">
          <img src="images/part0-200.jpg" style="width:100%; max-width:1600px; height:auto;" alt="part0-200">
          <div class="image-caption">Results for num_inference_steps = 200</div>
        </div>

        <div class="result-image">
          <img src="images/part0-both.jpg" style="width:100%; max-width:1600px; height:auto;" alt="part0-both">
          <div class="image-caption">Results for num_inference_steps = 20 (stage 1) & 200 (stage 2)</div>
        </div>
      </div>
      
    </section>

    
    <section>
      <h2>A.1.1: Implementing the Forward Process</h2>
      <p>Next, we move on to sampling loops by making use of the pretrained DeepFloyd denoisers. In this first subpart, we prepare ourselves for denoising by iteratively adding noise to a clean image. That is, given a clean image, we sample from a Gaussian (using mean and variance), apply scaling, and get a noisy image at a particular timestep. This process is stored in a "forward" function, and here are the results on an image of the campanile:</p>

        
      <div class="image-grid">
        <div class="result-image">
          <img src="images/campanile-noises.jpg" style="width:100%; max-width:1600px; height:auto;" alt="campanile-noises">
          <div class="image-caption">Campanile at various noise levels</div>
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>A.1.2: Classical Denoising</h2>
      <p>Next, we are ready for denoising. In classical denoising, we use Gaussian blur filtering to try to remove noise. After applying forward on our images, we use torchvision.transforms.functional.gaussian_blur to apply the gaussian blur filter. Here I used a kernel size of 5, and as we can see, the image looks smoother but the essence of the campanile remains destroyed:</p>
      

      <div class="image-grid">
        <div class="result-image">
          <img src="images/campanile-noises.jpg" style="width:100%; max-width:1600px; height:auto;" alt="campanile-noises">
        </div>
        
        <div class="result-image">
          <img src="images/campanile-gausblur.jpg" style="width:100%; max-width:1600px; height:auto;" alt="campanile-gausblur">
          <div class="image-caption">Campanile with Gaussian denoise applied</div>
        </div>
        
      </div>


      
    </section>

     <section>
      <h2>A.1.3: One-Step Denoising</h2>
      <p>Then, we use a pretrained diffusion model (stage_1.unet) to denoise. Aside from applying forward to our campanile image, we estimate the noise in the new noisy image by passing it through stage_1.unet. Using this, we can then remove noise from the noisy image to obtain an estimate of the original (clean) image. Importantly, we don't remove the noisy directly; we must scale the noise before removing since the forward process adds a *scaled* version of the noise. The results do no disappoint:</p>

      <div class="image-grid">
        <div class="result-image">
          <img src="images/campanile.jpg" alt="original-campanile">
          <div class="image-caption">Original Campanile Photo</div>
        </div>
        
        <div class="result-image">
          <img src="images/campanile-onestep.jpg" style="width:100%; max-width:1600px; height:auto;" alt="campanile-onestep">
          <div class="image-caption">Campanile with one-step denoising</div>
        </div>
        
      </div>
       
    </section>

    
    <section>
      <h2>A.1.4: Iterative Denoising</h2>
      <p>Although 1.3 is great, we can see that the quality of the results deteriorate as the images become noisier. To address this, we try denoising in an iterative manner -- using timesteps. However, we do not want to go through all steps from 1 to n as that would be super slow, so we take our steps in strides of 30. This is done by creating a function that denoises our image at some starting timestep index (timestep[i_start]), applying a mathy formula to obtain an image at the next timestep (t' = timestep[i_start + 1]), and repeating the process until we produce a clean image. Here are some figures for comparison:</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/campanile-iter5.jpg" style="width:100%; max-width:1600px; height:auto;" alt="campanile-iter5">
        </div>
        
        <div class="result-image">
          <img src="images/campanile-iter-results.jpg" style="width:100%; max-width:1600px; height:auto;" alt="campanile-iter-results">
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>A.1.5: Diffusion Model Sampling</h2>
      <p>Aside from denoising images, we can also use the denoising model to generate images from scratch. This is done by setting our starting index / timestep as 0 and in our noisy image as random noise, which in effect denoises pure noise. To visualize the results, we take 5 samples of the prompt "a high quality photo".</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/diffusion-samples.jpg" style="width:100%; max-width:1600px; height:auto;" alt="diffusion-samples">
          <div class="image-caption">Diffusion Sample Results</div>
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>A.1.6: Classifier-Free Guidance (CFG)</h2>
      <p>The results in 1.5 were not great, but we can improve this with CFGs! In CFG, we compute both a conditional and unconditional noise estimate then scale their difference and sum them up to "condense" them to a sinlge noise. Our scale controls the strength of the CFG (how closely we want our generated images to follow our text prompt). These are the results on the same "high quality photo" prompt using a scale value of 7. For all subsequent subparts, we use CFGs.</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/cfgs.jpg" style="width:100%; max-width:1600px; height:auto;" alt="cfgs">
          <div class="image-caption">CFG Results</div>
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>A.1.7: Image-to-image Translation</h2>
      <p>Now that we have CFGs, we can get a little more creative! Here, we generate an image that is similar to the campanile by applying a series of edits to a random nose-generated image. We then take the original campanile image, add a little bit of noise using forward, and run iterative_denoise_cfg from 1.6. This produces a series of edits to our random image, each step getting closer and closer to original campanile. We also try the process on some other images, producing really cool results!</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/im2im.jpg" style="width:100%; max-width:1600px; height:auto;" alt="im2im">
          <div class="image-caption">Campanile Edits</div>
        </div>

        <div class="result-image">
          <img src="images/grad-back.png" alt="grad-back">
          <div class="image-caption">Reference image (grad photo)</div>
        </div>
        
        <div class="result-image">
          <img src="images/im2im-other1.jpg" style="width:100%; max-width:1600px; height:auto;" alt="im2im-other1">
          <div class="image-caption">Edits on grad image</div>
        </div>

        <div class="result-image">
          <img src="images/bear.jpg" alt="bear">
          <div class="image-caption">Reference image (bear)</div>
        </div>

        <div class="result-image">
          <img src="images/im2im-other2.jpg" style="width:100%; max-width:1600px; height:auto;" alt="im2im-other2">
          <div class="image-caption">Edits on bear image</div>
        </div>
        
      </div>

      <h4>1.7.1: Editing Hand-Drawn and Web Images</h4>
      <p>We can also apply the process on illustrations! Here are the edits made on web and hand drawn images.</p>

      <div class="image-grid">
        <div class="result-image">
          <img src="images/web-original.jpg" alt="web-original">
          <div class="image-caption">Original Sundae Kids Illustration (Web)</div>
        </div>

        <div class="result-image">
          <img src="images/web-drawn.jpg" style="width:100%; max-width:1600px; height:auto;" alt="web-drawn">
          <div class="image-caption">Sundae Kids Illustration Edit Result</div>
        </div>

        
        <div class="result-image">
          <img src="images/hand1-original.png" alt="hand1-original">
          <div class="image-caption">Original Doodle #1 (Hand-Drawn)</div>
        </div>

        <div class="result-image">
          <img src="images/hand-drawn1.jpg" style="width:100%; max-width:1600px; height:auto;" alt="hand-drawn1">
          <div class="image-caption">Doodle #1 Edit Result</div>
        </div>

        <div class="result-image">
          <img src="images/hand2-original.png" alt="hand2-original">
          <div class="image-caption">Original Doodle #2 (Hand-Drawn)</div>
        </div>

        <div class="result-image">
          <img src="images/hand-drawn2.jpg" style="width:100%; max-width:1600px; height:auto;" alt="hand-drawn2">
          <div class="image-caption">Doodle #2 Edit Result</div>
        </div>
      </div>
      
      
      <h4>1.7.2: Inpainting</h4>
      <p>Another cool application of this is image inpainting. Here are the results on a few images (campanile, rose, grad photo scene from Berkeley's cherry blossom trees).</p>


      <div class="image-grid">
        <div class="result-image">
          <img src="images/impaint-camp.jpg" alt="inpaint-camp">
        </div>

        <div class="result-image">
          <img src="images/impaint-rose.jpg"  alt="inpaint-rose">
        </div>

        <div class="result-image">
          <img src="images/impaint-grad.jpg" alt="inpaint-grad">
        </div>
      </div>

        
      <h4>1.7.3: Text-Conditional Image-to-image Translation</h4>
      <p>Aside from passing in "a high quality photo" as our prompt, we can give our model a specific prompt, and as we can see, it will make edits on an image that matches our prompt instead of a random image. By doing so we are essentially guiding our model with a text prompt. Here are some fun results:</p>

      <div class="image-grid">
        <div class="result-image">
          <img src="images/textcond-camp.jpg" style="width:100%; max-width:1600px; height:auto;" alt="textcond-camp">
          <div class="image-caption">Results on Campanile (from woman in black dress)</div>
        </div>

        <div class="result-image">
          <img src="images/textcond-rose.jpg" style="width:100%; max-width:1600px; height:auto;" alt="textcond-rose">
          <div class="image-caption">Results on rose (from plate of pasta)</div>
        </div>

        <div class="result-image">
          <img src="images/textcond-bear.jpg" style="width:100%; max-width:1600px; height:auto;" alt="textcond-bear">
          <div class="image-caption">Results on bear (from countryside sunset)</div>
        </div>
      </div>
      
      
    </section>


    <section>
      <h2>A.1.8: Visual Anagrams</h2>
      <p>Another cool application of CFGs is that it allows us to create optical illusions. Here we create anagrams by denoising an image normally using one prompt, giving us a noise estimate e1, then flipping the image upside down and denoising it with another prompt to get a second noise estimate e2. Then we flip e2 back and take the average of the two noises so that we have a noise e that captures the essence of both prompts.
        Below are anagram results for prompts "a nice view in Italy" and "a plate of pasta" (top set) as well as "a lithograph of a Japanese airport" and "a painting of cherry blossoms" (bottom set).</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/anagram-pasta.jpg"  style="width:600px;" alt="anagram-pasta">
          <div class="image-caption">Anagram of Italy and pasta</div>
        </div>
        
        <div class="result-image">
          <img src="images/anagram-japan.jpg"  style="width:600px;" alt="anagram-japan">
          <div class="image-caption">Anagram of Japanese airport and cherry blossoms</div>
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>A.1.9: Hybrid Images</h2>
      <p>Lastly, we experiment with hybrid images by again creating composite noise estimates. This is done by combining the low frequencies of one noise estimate with the high frequencies of another. Below are hyrid results for prompts ______________________.</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/board-left.jpg" style="width:350px;" alt="board-left">
          <div class="image-caption">Menu - Left View</div>
        </div>
        
        <div class="result-image">
          <img src="images/board-right.jpg" style="width:350px;" alt="board-right">
          <div class="image-caption">Menu - Right View</div>
        </div>
        
      </div>
      
    </section>

    
    
    <!-- ---------------------------------------------------------------------------------------------------------------------------------------------- -->
    <hr class="gradient">



    
    <section>
      <h2>B.1.1: Implementing the UNet</h2>
      <!-- NO DELIVERABLES HERE -->
      <p>In part B we try out alternative denoising and image sampling techniques by turning to UNets and flow matching. In this first part, we build a one-step denoiser with the following network architecture and specifications:
        <ul>
          <li>Conv2d(kernel_size, stride, padding) is nn.Conv2d()</li>
          <li>BN is nn.BatchNorm2d()</li>
          <li>GELU is nn.GELU()</li>
          <li>ConvTranspose2d(kernel_size, stride, padding) is nn.ConvTranspose2d()</li>
          <li>AvgPool(kernel_size) is nn.AvgPool2d()</li>
          <li>D is the number of hidden channels and is a hyperparameter that we will set ourselves.</li>
        </ul>
      </p>

      
      <p>At a high level, the blocks do the following:
        <ul>
          <li>(1) Conv is a convolutional layer that doesn't change the image resolution, only the channel dimension.</li>
          <li>(2) DownConv is a convolutional layer that downsamples the tensor by 2.</li>
          <li>(3) UpConv is a convolutional layer that upsamples the tensor by 2.</li>
          <li>(4) Flatten is an average pooling layer that flattens a 7x7 tensor into a 1x1 tensor. 7 is the resulting height and width after the downsampling operations.</li>
          <li>(5) Unflatten is a convolutional layer that unflattens/upsamples a 1x1 tensor into a 7x7 tensor.</li>
          <li>(6) Concat is a channel-wise concatenation between tensors with the same 2D shape. This is simply torch.cat()</li>
        </ul>
      </p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/unet.png" style="width:100%; max-width:1600px; height:auto;"  alt="unet">
          <div class="image-caption">(Unconditional) UNet Architecture</div>
        </div>

        <div class="result-image">
          <img src="images/operations.png" style="width:100%; max-width:1600px; height:auto;" alt="operations">
          <div class="image-caption">UNet Operations</div>
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>B.1.2: Using the UNet to Train a Denoiser</h2>
      <p>After building our network, we can formulate a denoising objective: given some noisy image z, we'd like to train a denoiser D that can map z to a clean image x. Similar to part A, we start by adding noise to our clean images (MNIST digits) by generating training data pairs (z, x) with z = x + σe where e follows the Normal distribution from 0 to our image I. Here are the results across various σ values and digits:</p>

      <div class="image-grid">
        <div class="result-image">
          <img src="images/1.2.jpg" style="width:450px;" alt="1.2">
        </div>

      </div>
      

      <h4>1.2.1: Training</h4>
      <p>Afterwards, we can train a denoiser D. Here we train with σ=0.5, a batch size of 256, 5 epochs, UNet with hidden dimension of 128, and Adam optimizer with 1e-4 learning rate.</p>
      <div class="image-grid">
        <div class="result-image">
          <img src="images/1.2.1-epoch.jpg" style="height:300px; width:auto;" alt="1.2.1-epoch">
          <div class="image-caption">Sample results on test set</div>
        </div>
        
        <div class="result-image">
          <img src="images/1.2.1-curve.jpg" style="height:300px; width:auto;" alt="1.2.1-curve">
          <div class="image-caption">Training loss curve</div>
        </div>

      </div>

      
      <h4>1.2.2: Out-of-Distribution Testing</h4>
      <p>Aside from visualizing the results on our trained sigma value of 0.5, we can also examine the results on other sigma values that it wasn't trained on. As we can observe, it seems to perform worse on larger sigma values.</p>
      <div class="image-grid">
        <div class="result-image">
          <img src="images/1.2.2.jpg" style="width:550px;" alt="1.2.2">
        </div>
 
      </div>

      
      <h4>1.2.3: Denoising Pure Noise</h4>
      <p>We can also denoise pure noise by passing in a "blank canvas" of z = e. This is essentially the same process as 1.2.1 but instead of passing in digit images we use pure noise. As we can observe, the outputs look somewhat ghost-like and pretty much the same across all of our noise inputs. Intuitively, this is because we are computing the "average" of output digits 0-9 as there is no digit it is conditioned on.</p>
      <div class="image-grid">
        <div class="result-image">
          <img src="images/1.2.3-epoch.jpg" style="height:270px; width:auto;" alt="1.2.3-epoch">
          <div class="image-caption">Sample results on test set</div>
        </div>
        
        <div class="result-image">
          <img src="images/1.2.3-curve.jpg" style="height:270px; width:auto;" alt="1.2.3-curve">
          <div class="image-caption">Training loss curve for pure noise</div>
        </div>

      </div>

      
      
    </section>
    

    
    <section>
      <h2>B.2.1: Adding Time Conditioning to UNet</h2>
      <p>As we just saw in 1.2.3, denoising does not work well with pure noise (generative tasks). To address this, we can leverage flow matching which will denoise our digits iteratively. This difference with this is that we train a UNet model that essentially predicts the 'flow' from our noisy data to clean data. Intermediate noise samples are constructed using linear interpolation. To introduce conditioning, we use time (denoted as t below).</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/time-unet.png" style="width:100%; max-width:1600px; height:auto;" alt="time-unet">
          <div class="image-caption">Time Conditional UNet</div>
        </div>
        
        <div class="result-image">
          <img src="images/fcblock.png" style="width:100%; max-width:1600px; height:auto;" alt="fcblock">
          <div class="image-caption">FCBlock (used for conditioning)</div>
        </div>
      </div>

    </section>



    <section>
      <h2>B.2.2: Training the UNet</h2>
      <p>Training is fairly straightforward here: after picking a random image from the training set and a random timestep, we can add noise to our random image to get xt. Then we train the denoiser to predict the flow at xt and repeat for different images and timesteps until the model converges. Now we use a batch size of 64, a hidden dimension of 64, Adam with 1e-2 learning rate, and a scheduler using torch.optim.lr_scheduler.ExponentialLR(...).</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/2.2.jpg" style="width:450px;" alt="2.2">
          <div class="image-caption">Training Loss Curve for Time-Conditioned UNet</div>
        </div>
        
      </div>
      
    </section>


    <section>
      <h2>B.2.3: Sampling from the UNet</h2>
      <p>___________________________________.</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/2.3.jpg" style="width:450px;" alt="2.3">
          <div class="image-caption">Sampled Results for Epochs 1, 5, 10</div>
        </div>
        

      </div>
      
    </section>

    
    <section>
      <h2>B.2.4: Adding Class-Conditioning to UNet</h2>
      <p>___________________________________.</p>
        
      
    </section>

    
    <section>
      <h2>B.2.5: Training the UNet</h2>
      <p>___________________________________.</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/2.5.jpg" style="width:450px;" alt="2.5">
          <div class="image-caption">Training Loss Curve for Class-Conditioned UNet</div>
        </div>
        
      </div>
      
    </section>

    
    <section>
      <h2>B.2.6: Sampling from the UNet</h2>
      <p>___________________________________.</p>
      
      <div class="image-grid">
        <div class="result-image">
          <img src="images/2.6.jpg" style="width:650px;" alt="2.6">
          <div class="image-caption">Sampled Results for Epochs 1, 5, 10</div>
        </div>

        <div class="result-image">
          <img src="images/2.6-noscheduler.jpg" style="width:650px;" alt="2.6-noscheduler">
          <div class="image-caption">Sampled Results for Epochs 1, 5, 10 (No Scheduler)</div>
        </div>

      
    </section>

    
  </main>

</body>
</html>
